{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7021593d-e567-4798-a8a2-92d45fbb431f",
      "metadata": {
        "id": "7021593d-e567-4798-a8a2-92d45fbb431f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Configure matplotlib backend based on environment\n",
        "if 'ipykernel' in sys.modules:\n",
        "    # Running in Jupyter/IPython\n",
        "    try:\n",
        "        import IPython\n",
        "        ipython = IPython.get_ipython()\n",
        "        ipython.run_line_magic('matplotlib', 'inline')\n",
        "    except Exception:\n",
        "        os.environ['MPLBACKEND'] = 'TkAgg'\n",
        "else:\n",
        "    # Running as script\n",
        "    os.environ['MPLBACKEND'] = 'TkAgg'\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)\n",
        "import glob\n",
        "from keras_tuner import RandomSearch\n",
        "import sys\n",
        "\n",
        "# Configuração otimizada para GPU NVIDIA/CUDA\n",
        "def setup_gpu():\n",
        "    print(\"\\nVerificando configuração GPU:\")\n",
        "    print(\"TensorFlow versão:\", tf.__version__)\n",
        "    print(\"CUDA disponível:\", tf.test.is_built_with_cuda())\n",
        "    print(\"GPU disponível para TensorFlow:\", tf.test.is_gpu_available())\n",
        "\n",
        "    try:\n",
        "        # Listar GPUs disponíveis\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            print(\"\\nGPUs disponíveis:\", len(gpus))\n",
        "            for gpu in gpus:\n",
        "                print(\" -\", gpu.name)\n",
        "\n",
        "            # Permitir crescimento de memória dinâmico\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "            # Configurar para formato de dados mixed precision\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "            print(\"\\nGPU configurada com sucesso!\")\n",
        "            print(\"Usando mixed precision:\", policy.name)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"\\nNenhuma GPU encontrada. Usando CPU.\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nErro ao configurar GPU:\", str(e))\n",
        "        print(\"Usando CPU como fallback.\")\n",
        "        return False\n",
        "\n",
        "# Configurar GPU no início do script\n",
        "using_gpu = setup_gpu()\n",
        "\n",
        "from tensorflow import keras, data\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import layers, regularizers, activations, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset_name = \"bearing_dataset\"  # Apenas para referência\n",
        "#train_ratio = 0.75\n",
        "row_mark = 740\n",
        "batch_size = 128\n",
        "time_step = 1\n",
        "x_dim = 4\n",
        "lstm_h_dim = 8\n",
        "z_dim = 4\n",
        "epoch_num = 100\n",
        "threshold = None\n",
        "\n",
        "mode = 'train'\n",
        "model_dir = \"./lstm_vae_model/\"\n",
        "image_dir = \"./lstm_vae_images/\"\n",
        "data_dir = r\"C:\\Users\\Augusto-PC\\Documents\\GitHub\\LSTM\\Temperaturas maximas\\Convencionais processadas temperaturas\"\n",
        "\n",
        "# Criar diretórios necessários\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Parâmetros de ativação\n",
        "lstm_activation = 'softplus'  # Pode mudar para 'tanh', 'relu', etc\n",
        "sigma_activation = 'tanh'     # Ativação para sigma_x\n",
        "\n",
        "def split_normalize_data(all_df):\n",
        "    #row_mark = int(all_df.shape[0] * train_ratio)\n",
        "    train_df = all_df[:row_mark]\n",
        "    test_df = all_df[row_mark:]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(np.array(all_df)[:, 1:])\n",
        "    train_scaled = scaler.transform(np.array(train_df)[:, 1:])\n",
        "    test_scaled = scaler.transform(np.array(test_df)[:, 1:])\n",
        "    return train_scaled, test_scaled\n",
        "\n",
        "def reshape(da):\n",
        "    return da.reshape(da.shape[0], time_step, da.shape[1]).astype(\"float32\")\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    def __init__(self, name='sampling_z'):\n",
        "        super(Sampling, self).__init__(name=name)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mu, logvar = inputs\n",
        "        print('mu: ', mu)\n",
        "        sigma = K.exp(logvar * 0.5)\n",
        "        epsilon = K.random_normal(shape=(mu.shape[0], z_dim), mean=0.0, stddev=1.0)\n",
        "        return mu + epsilon * sigma\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Sampling, self).get_config()\n",
        "        config.update({'name': self.name})\n",
        "        return config\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='encoder', activation=lstm_activation, **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "\n",
        "        self.encoder_inputs = keras.Input(shape=(time_step, x_dim))\n",
        "        self.encoder_lstm = layers.LSTM(\n",
        "            lstm_h_dim,\n",
        "            activation=activation,  # Usar parâmetro\n",
        "            name='encoder_lstm',\n",
        "            stateful=True\n",
        "        )\n",
        "        self.z_mean = layers.Dense(z_dim, name='z_mean')\n",
        "        self.z_logvar = layers.Dense(z_dim, name='z_log_var')\n",
        "        self.z_sample = Sampling()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.encoder_inputs = inputs\n",
        "        hidden = self.encoder_lstm(self.encoder_inputs)\n",
        "        mu_z = self.z_mean(hidden)\n",
        "        logvar_z = self.z_logvar(hidden)\n",
        "        z = self.z_sample((mu_z, logvar_z))\n",
        "        return mu_z, logvar_z, z\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Encoder, self).get_config()\n",
        "        config.update({\n",
        "            'name': self.name,\n",
        "            'z_sample': self.z_sample.get_config()\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='decoder', activation=lstm_activation, sigma_activation=sigma_activation, **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "\n",
        "        self.z_inputs = layers.RepeatVector(time_step, name='repeat_vector')\n",
        "        self.decoder_lstm_hidden = layers.LSTM(\n",
        "            lstm_h_dim,\n",
        "            activation=activation,\n",
        "            return_sequences=True,\n",
        "            name='decoder_lstm'\n",
        "        )\n",
        "        self.x_mean = layers.Dense(x_dim, name='x_mean')\n",
        "        self.x_sigma = layers.Dense(\n",
        "            x_dim,\n",
        "            name='x_sigma',\n",
        "            activation=sigma_activation  # Usar parâmetro\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z = self.z_inputs(inputs)\n",
        "        hidden = self.decoder_lstm_hidden(z)\n",
        "        mu_x = self.x_mean(hidden)\n",
        "        sigma_x = self.x_sigma(hidden)\n",
        "        return mu_x, sigma_x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Decoder, self).get_config()\n",
        "        config.update({\n",
        "            'name': self.name\n",
        "        })\n",
        "        return config\n",
        "\n",
        "loss_metric = keras.metrics.Mean(name='loss')\n",
        "likelihood_metric = keras.metrics.Mean(name='log likelihood')\n",
        "\n",
        "class LSTM_VAE(keras.Model):\n",
        "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='lstm_vae', **kwargs):\n",
        "        super(LSTM_VAE, self).__init__(name=name, **kwargs)\n",
        "\n",
        "        self.encoder = Encoder(time_step, x_dim, lstm_h_dim, z_dim, **kwargs)\n",
        "        self.decoder = Decoder(time_step, x_dim, lstm_h_dim, z_dim, **kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mu_z, logvar_z, z = self.encoder(inputs)\n",
        "        mu_x, sigma_x = self.decoder(z)\n",
        "\n",
        "        var_z = K.exp(logvar_z)\n",
        "        kl_loss = K.mean(-0.5 * K.sum(var_z - logvar_z + tf.square(1 - mu_z), axis=1), axis=0)\n",
        "        self.add_loss(kl_loss)\n",
        "\n",
        "        dist = tfp.distributions.Normal(loc=mu_x, scale=tf.abs(sigma_x))\n",
        "        log_px = -dist.log_prob(inputs)\n",
        "\n",
        "        return mu_x, sigma_x, log_px\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'encoder': self.encoder.get_config(),\n",
        "            'decoder': self.decoder.get_config(),\n",
        "            'name': self.name\n",
        "        }\n",
        "        return config\n",
        "\n",
        "    def reconstruct_loss(self, x, mu_x, sigma_x):\n",
        "        var_x = K.square(sigma_x)\n",
        "        reconst_loss = -0.5 * K.sum(K.log(var_x), axis=2) + K.sum(K.square(x - mu_x) / var_x, axis=2)\n",
        "        reconst_loss = K.reshape(reconst_loss, shape=(x.shape[0], 1))\n",
        "        return K.mean(reconst_loss, axis=0)\n",
        "\n",
        "    def mean_log_likelihood(self, log_px):\n",
        "        log_px = K.reshape(log_px, shape=(log_px.shape[0], log_px.shape[2]))\n",
        "        mean_log_px = K.mean(log_px, axis=1)\n",
        "        return K.mean(mean_log_px, axis=0)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.device('/GPU:0' if using_gpu else '/CPU:0'):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Usar tf.data para otimizar o pipeline de dados\n",
        "                if isinstance(data, tf.data.Dataset):\n",
        "                    x = next(iter(data))\n",
        "                else:\n",
        "                    x = data\n",
        "\n",
        "                # Forward pass com mixed precision\n",
        "                if using_gpu:\n",
        "                    x = tf.cast(x, tf.float16)\n",
        "\n",
        "                # Forward pass\n",
        "                mu_z, logvar_z = self.encoder(x)\n",
        "                z = self.encoder.z_sample([mu_z, logvar_z])\n",
        "                mu_x, sigma_x = self.decoder(z)\n",
        "\n",
        "                # Calcular loss\n",
        "                log_px = self.reconstruct_loss(x, mu_x, sigma_x)\n",
        "                kl_loss = -0.5 * tf.reduce_mean(1 + logvar_z - tf.square(mu_z) - tf.exp(logvar_z))\n",
        "                loss = -log_px + kl_loss\n",
        "\n",
        "                # Escalar loss para mixed precision\n",
        "                if using_gpu:\n",
        "                    loss = tf.cast(loss, tf.float32)\n",
        "\n",
        "                # Atualizar métricas\n",
        "                loss_metric.update_state(loss)\n",
        "                likelihood_metric.update_state(log_px)\n",
        "\n",
        "            # Backpropagation otimizado\n",
        "            variables = self.trainable_variables\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "            # Clipar gradientes para evitar explosão\n",
        "            gradients = [tf.clip_by_norm(g, 1.0) if g is not None else g for g in gradients]\n",
        "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "            return {\n",
        "                \"loss\": loss_metric.result(),\n",
        "                \"log_px\": likelihood_metric.result(),\n",
        "                \"kl_loss\": kl_loss\n",
        "            }\n",
        "\n",
        "def prepare_training_data(all_df, batch_size=128):  # Aumentado batch_size para GPU\n",
        "    # Preparar dados usando tf.data para pipeline otimizado\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(all_df.values.astype('float32'))\n",
        "\n",
        "    # Configurar pipeline otimizado\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.shuffle(buffer_size=min(len(all_df), 10000))  # Buffer size limitado para memória\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    if using_gpu:\n",
        "        # Usar estratégia de distribuição para GPU\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "        dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data if no Excel files are found.\"\"\"\n",
        "    print(\"\\nGenerating sample data for testing...\")\n",
        "\n",
        "    # Create sample timestamps\n",
        "    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='H')\n",
        "    n_samples = len(dates)\n",
        "\n",
        "    # Generate synthetic data\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        'timestamp': dates,\n",
        "        'nivel_agua': np.random.normal(100, 10, n_samples),  # Normal distribution\n",
        "        'vazao': np.abs(np.random.normal(50, 5, n_samples)),  # Positive values\n",
        "        'temperatura': np.random.normal(25, 3, n_samples),  # Normal distribution\n",
        "    }\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Add some anomalies\n",
        "    anomaly_idx = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
        "    df.loc[anomaly_idx, 'nivel_agua'] *= 1.5\n",
        "    df.loc[anomaly_idx, 'vazao'] *= 2\n",
        "    df.loc[anomaly_idx, 'temperatura'] += 10\n",
        "\n",
        "    # Save to Excel file\n",
        "    sample_file = os.path.join(data_dir, 'sample_data.xlsx')\n",
        "    print(f\"Saving sample data to {sample_file}\")\n",
        "    df.to_excel(sample_file, index=False)\n",
        "    return df\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    print(\"Loading and preparing data...\")\n",
        "\n",
        "    # Diretório onde estão os arquivos Excel\n",
        "    dfs = []\n",
        "\n",
        "    # Get list of Excel files (ignorando arquivos temporários que começam com ~$)\n",
        "    excel_files = [f for f in glob.glob(os.path.join(data_dir, \"*.xlsx\")) if not os.path.basename(f).startswith('~$')]\n",
        "\n",
        "    if not excel_files:\n",
        "        print(f\"\\nWARNING: No Excel files found in {data_dir}\")\n",
        "        print(\"Please make sure the Excel files are in the correct directory.\")\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        # Load and combine data from all files\n",
        "        for file in excel_files:\n",
        "            print(f\"Loading {os.path.basename(file)}...\")\n",
        "            try:\n",
        "                df = pd.read_excel(file)\n",
        "\n",
        "                # Verificar se as colunas necessárias existem\n",
        "                required_cols = ['Dia', 'Mes', 'Ano']\n",
        "                missing = [col for col in required_cols if col not in df.columns]\n",
        "\n",
        "                # Procurar a coluna de temperatura (pode ter variações no nome)\n",
        "                temp_col = None\n",
        "                for col in df.columns:\n",
        "                    if 'TEMPERATURA MAXIMA' in str(col).upper():\n",
        "                        temp_col = col\n",
        "                        break\n",
        "\n",
        "                if temp_col is None:\n",
        "                    print(f\"Aviso: {os.path.basename(file)} não possui coluna de temperatura máxima\")\n",
        "                    continue\n",
        "\n",
        "                if missing:\n",
        "                    print(f\"Aviso: {os.path.basename(file)} está faltando colunas {missing}\")\n",
        "                    continue\n",
        "\n",
        "                # Criar coluna de data\n",
        "                df['Data'] = pd.to_datetime({\n",
        "                    'year': df['Ano'],\n",
        "                    'month': df['Mes'],\n",
        "                    'day': df['Dia']\n",
        "                })\n",
        "\n",
        "                # Adicionar identificador da estação\n",
        "                station_id = os.path.splitext(os.path.basename(file))[0]\n",
        "                df['station_id'] = station_id\n",
        "\n",
        "                # Renomear coluna de temperatura para um nome mais simples\n",
        "                df = df.rename(columns={temp_col: 'temperatura'})\n",
        "\n",
        "                # Remover linhas com valores nulos na temperatura\n",
        "                df = df.dropna(subset=['temperatura'])\n",
        "\n",
        "                dfs.append(df)\n",
        "                print(f\"Processado com sucesso: {len(df)} registros\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao ler {file}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"Nenhum arquivo válido para processar\")\n",
        "\n",
        "    # Combine all dataframes\n",
        "    print(\"Combining data from all stations...\")\n",
        "    all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Extrair características temporais\n",
        "    all_df['year'] = all_df['Data'].dt.year\n",
        "    all_df['month'] = all_df['Data'].dt.month\n",
        "    all_df['day'] = all_df['Data'].dt.day\n",
        "    all_df['dayofweek'] = all_df['Data'].dt.dayofweek\n",
        "    all_df['is_weekend'] = all_df['Data'].dt.dayofweek >= 5\n",
        "\n",
        "    # Selecionar apenas as colunas numéricas relevantes para o modelo\n",
        "    feature_cols = ['temperatura', 'year', 'month', 'day', 'dayofweek', 'is_weekend']\n",
        "    all_df = all_df[feature_cols]\n",
        "\n",
        "    # Update x_dim based on actual number of features\n",
        "    global x_dim\n",
        "    x_dim = len(feature_cols)\n",
        "    print(f\"Number of features (x_dim): {x_dim}\")\n",
        "    print(f\"Total number of records: {len(all_df)}\")\n",
        "\n",
        "    return all_df\n",
        "\n",
        "def hyperparameter_tuning(train_X):\n",
        "    def build_model(hp):\n",
        "        # Parâmetros ajustáveis\n",
        "        hp_lstm_dim = hp.Int('lstm_dim', min_value=4, max_value=32, step=4)\n",
        "        hp_z_dim = hp.Int('z_dim', min_value=2, max_value=16, step=2)\n",
        "        hp_learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "        model = LSTM_VAE(\n",
        "            time_step,\n",
        "            train_X.shape[2],\n",
        "            hp_lstm_dim,\n",
        "            hp_z_dim\n",
        "        )\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(\n",
        "                learning_rate=hp_learning_rate,\n",
        "                amsgrad=True\n",
        "            )\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    tuner = RandomSearch(\n",
        "        build_model,\n",
        "        objective='loss',\n",
        "        max_trials=10,\n",
        "        executions_per_trial=2,\n",
        "        directory='tuner_results',\n",
        "        project_name='lstm_vae_tuning'\n",
        "    )\n",
        "\n",
        "    print(\"\\nIniciando busca de hiperparâmetros...\")\n",
        "    tuner.search(train_X, epochs=50, batch_size=batch_size)\n",
        "\n",
        "    print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
        "    print(tuner.get_best_hyperparameters(num_trials=1)[0].values)\n",
        "\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    return best_model\n",
        "\n",
        "def plot_loss_moment(history):\n",
        "    _, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
        "    ax.plot(history['loss'], 'blue', label='Loss', linewidth=1)\n",
        "    ax.plot(history['log_likelihood'], 'red', label='Log likelihood', linewidth=1)\n",
        "    ax.set_title('Loss and log likelihood over epochs')\n",
        "    ax.set_ylabel('Loss and log likelihood')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.legend(loc='upper right')\n",
        "    plt.savefig(image_dir + 'loss_lstm_vae_' + mode + '.png')\n",
        "\n",
        "def plot_log_likelihood(df_log_px):\n",
        "    plt.figure(figsize=(14, 6), dpi=80)\n",
        "    plt.title(\"Log likelihood\")\n",
        "    sns.set_color_codes()\n",
        "    sns.distplot(df_log_px, bins=40, kde=True, rug=True, color='blue')\n",
        "    plt.savefig(image_dir + 'log_likelihood_' + mode + '.png')\n",
        "\n",
        "def save_model(model):\n",
        "    with open(model_dir + 'lstm_vae.json', 'w') as f:\n",
        "        f.write(model.to_json())\n",
        "    model.save_weights(model_dir + 'lstm_vae_ckpt')\n",
        "\n",
        "def load_model():\n",
        "    lstm_vae_obj = {'Encoder': Encoder, 'Decoder': Decoder, 'Sampling': Sampling}\n",
        "    with keras.utils.custom_object_scope(lstm_vae_obj):\n",
        "        with open(model_dir + 'lstm_vae.json', 'r'):\n",
        "            model = keras.models.model_from_json(model_dir + 'lstm_vae.json')\n",
        "        model.load_weights(model_dir + 'lstem_vae_ckpt')\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    print(\"\\nIniciando processamento...\")\n",
        "    if using_gpu:\n",
        "        print(\"Executando com otimização de GPU\")\n",
        "    else:\n",
        "        print(\"Executando com configuração padrão\")\n",
        "\n",
        "    # Data loading\n",
        "    print(\"Loading and preparing data...\")\n",
        "    all_df = load_and_prepare_data()\n",
        "\n",
        "    # Pre-processing\n",
        "    print(\"Pre-processing data...\")\n",
        "    train_scaled, test_scaled = split_normalize_data(all_df)\n",
        "    print(\"train and test data shape after scaling: \", train_scaled.shape, test_scaled.shape)\n",
        "\n",
        "    train_X = reshape(train_scaled)\n",
        "    test_X = reshape(test_scaled)\n",
        "\n",
        "    if mode == \"train\":\n",
        "        print(\"\\nIniciando otimização de hiperparâmetros...\")\n",
        "        model = hyperparameter_tuning(train_X)\n",
        "\n",
        "        # Treinar com os melhores hiperparâmetros\n",
        "        history = model.fit(\n",
        "            train_X,\n",
        "            epochs=epoch_num,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False\n",
        "        ).history\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        # Calcular threshold dinâmico (95º percentil dos erros de treino)\n",
        "        _, _, train_log_px = model.predict(train_X)\n",
        "        train_errors = np.mean(train_log_px, axis=(1,2))\n",
        "        global threshold\n",
        "        threshold = np.percentile(train_errors, 95)  # 95% dos dados normais\n",
        "\n",
        "        # Salvar threshold junto com o modelo\n",
        "        np.save(os.path.join(model_dir, 'threshold.npy'), threshold)\n",
        "        print(f\"Auto Threshold: {threshold:.4f}\")\n",
        "\n",
        "        plot_loss_moment(history)\n",
        "        save_model(model)\n",
        "\n",
        "    elif mode == \"infer\":\n",
        "        model = load_model()\n",
        "        model.compile(optimizer=optimizers.Adam(learning_rate=0.001, epsilon=1e-6, amsgrad=True))\n",
        "\n",
        "        # Carregar threshold salvo\n",
        "        threshold = np.load(os.path.join(model_dir, 'threshold.npy'))\n",
        "    else:\n",
        "        print(\"Unknown mode: \", mode)\n",
        "        exit(1)\n",
        "\n",
        "    _, _, train_log_px = model.predict(train_X, batch_size=1)\n",
        "    train_log_px = train_log_px.reshape(train_log_px.shape[0], train_log_px.shape[2])\n",
        "    df_train_log_px = pd.DataFrame()\n",
        "    df_train_log_px['log_px'] = np.mean(train_log_px, axis=1)\n",
        "    plot_log_likelihood(df_train_log_px)\n",
        "\n",
        "\n",
        "    _, _, test_log_px = model.predict(test_X, batch_size=1)\n",
        "    test_log_px = test_log_px.reshape(test_log_px.shape[0], test_log_px.shape[2])\n",
        "    df_log_px = pd.DataFrame()\n",
        "    df_log_px['log_px'] = np.mean(test_log_px, axis=1)\n",
        "    df_log_px = pd.concat([df_train_log_px, df_log_px])\n",
        "    df_log_px['threshold'] = threshold\n",
        "    df_log_px['anomaly'] = df_log_px['log_px'] > df_log_px['threshold']\n",
        "    df_log_px.index = np.array(all_df)[:, 0]\n",
        "\n",
        "    df_log_px.plot(logy=True, figsize=(16, 9), color=['blue', 'red'])\n",
        "    plt.savefig(image_dir + 'anomaly_lstm_vae_' + mode + '.png')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "source": [
        "data_dir = r\"C:\\F_analises\\INMET\\Convencionais processadas temperaturas\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JJ24qa--UcIS"
      },
      "id": "JJ24qa--UcIS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}